{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
      "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
      "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
      "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
      "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
      "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
      "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
      "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
      "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "5  0.105915  0.253844  0.081080    3.67      0  \n",
      "6 -0.257237  0.034507  0.005168    4.99      0  \n",
      "7 -0.051634 -1.206921 -1.085339   40.80      0  \n",
      "8 -0.384157  0.011747  0.142404   93.20      0  \n",
      "9  0.094199  0.246219  0.083076    3.68      0  \n",
      "\n",
      "[10 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 8\n",
    "n_layers = 1\n",
    "\n",
    "dev = qml.device('lightning.gpu', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qlayer(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=list(np.arange(n_qubits)), rotation='X')\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(n_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 128)\n",
    "        self.fc2 = nn.Linear(128, n_qubits)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = nn.Parameter(torch.rand(n_layers, n_qubits, 3) * 1e-3)\n",
    "        self.output_layer = nn.Linear(n_qubits, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(x) * self.scale\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, n_qubits), dtype=torch.float32, device=x.device)\n",
    "        for batch_idx in range(batch_size):\n",
    "            expvals = qlayer(x[batch_idx], self.qnn_weights)\n",
    "            out[batch_idx] = torch.stack(expvals)\n",
    "        x = self.output_layer(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 128)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "        self.output_layer = nn.Linear(8, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_for_each_class_classical = 5\n",
    "batch_size_classical = 32\n",
    "\n",
    "X_train_sampled_classical = []\n",
    "y_train_sampled_classical = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class_classical, replace=False)\n",
    "\n",
    "    X_train_sampled_classical.extend(X_train[sampled_indices])\n",
    "    y_train_sampled_classical.extend(y_train[sampled_indices]) \n",
    "\n",
    "X_train_sampled_classical = np.array(X_train_sampled_classical)\n",
    "y_train_sampled_classical = np.array(y_train_sampled_classical)\n",
    "\n",
    "X_train_sampled_torch_classical = torch.tensor(X_train_sampled_classical, dtype=torch.float32)\n",
    "y_train_sampled_torch_classical = torch.tensor(y_train_sampled_classical, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset_classical = TensorDataset(X_train_sampled_torch_classical, y_train_sampled_torch_classical)\n",
    "train_sampled_loader_classical = DataLoader(train_sampled_dataset_classical, batch_size=batch_size_classical, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_for_each_class_quantum = 5\n",
    "batch_size_quantum = 32\n",
    "\n",
    "X_train_sampled_quantum = []\n",
    "y_train_sampled_quantum = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class_quantum, replace=False)\n",
    "\n",
    "    X_train_sampled_quantum.extend(X_train[sampled_indices])\n",
    "    y_train_sampled_quantum.extend(y_train[sampled_indices]) \n",
    "\n",
    "X_train_sampled_quantum = np.array(X_train_sampled_quantum)\n",
    "y_train_sampled_quantum = np.array(y_train_sampled_quantum)\n",
    "\n",
    "X_train_sampled_torch_quantum = torch.tensor(X_train_sampled_quantum, dtype=torch.float32)\n",
    "y_train_sampled_torch_quantum = torch.tensor(y_train_sampled_quantum, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset_quantum = TensorDataset(X_train_sampled_torch_quantum, y_train_sampled_torch_quantum)\n",
    "train_sampled_loader_quantum = DataLoader(train_sampled_dataset_quantum, batch_size=batch_size_quantum, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantum = HybridClassifier()\n",
    "\n",
    "learning_rate_quantum = 1e-3\n",
    "batch_size_quantum = 32\n",
    "epochs_quantum = 3\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop_quantum(dataloader, loss_fn, optimizer):\n",
    "    model_quantum.train()\n",
    "    model_quantum.to(torch_device)  \n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model_quantum(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def test_loop_quantum(dataloader, loss_fn):\n",
    "    model_quantum.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model_quantum.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model_quantum(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size_quantum) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classical = ClassicalClassifier()\n",
    "\n",
    "learning_rate_classical = 1e-3\n",
    "batch_size_classical = 32\n",
    "epochs_classical = 20\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop_classical(dataloader, loss_fn, optimizer):\n",
    "    model_classical.train()\n",
    "    model_classical.to(torch_device)  \n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model_classical(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def test_loop_classical(dataloader, loss_fn):\n",
    "    model_classical.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model_classical.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model_classical(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size_classical) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data for classical\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 67.91128540039062\n",
      "Avg loss: 0.667476 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 71.08892822265625\n",
      "Avg loss: 0.661653 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 74.12093353271484\n",
      "Avg loss: 0.655290 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 77.27049255371094\n",
      "Avg loss: 0.648201 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 79.7024154663086\n",
      "Avg loss: 0.641866 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 82.02555084228516\n",
      "Avg loss: 0.635391 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 84.41535949707031\n",
      "Avg loss: 0.628393 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 86.84552001953125\n",
      "Avg loss: 0.620529 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 88.80720520019531\n",
      "Avg loss: 0.613202 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 90.38812255859375\n",
      "Avg loss: 0.606320 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 91.73042297363281\n",
      "Avg loss: 0.599642 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 92.76741027832031\n",
      "Avg loss: 0.593137 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 93.77632141113281\n",
      "Avg loss: 0.586208 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 94.64662170410156\n",
      "Avg loss: 0.578851 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 95.43093872070312\n",
      "Avg loss: 0.571063 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 96.08541870117188\n",
      "Avg loss: 0.562923 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 96.67497253417969\n",
      "Avg loss: 0.554629 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 97.15925598144531\n",
      "Avg loss: 0.546163 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 97.58036804199219\n",
      "Avg loss: 0.537593 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 97.89794921875\n",
      "Avg loss: 0.528901 \n",
      "\n",
      "Training took 0.15 minutes.\n",
      "Accuracy: 97.89794921875\n",
      "Avg loss: 0.528901 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5289009970470067"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_classical.parameters(), lr=learning_rate_classical)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data for classical\\n--------------------------\")\n",
    "for t in range(epochs_classical):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_classical(train_sampled_loader_classical, loss_fn, optimizer)\n",
    "    test_loss = test_loop_classical(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_classical(test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training for classical\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 99.88243865966797\n",
      "Avg loss: 0.002916 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 99.87718200683594\n",
      "Avg loss: 0.003066 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 99.87718200683594\n",
      "Avg loss: 0.003122 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 99.88069152832031\n",
      "Avg loss: 0.002804 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 99.87718200683594\n",
      "Avg loss: 0.002743 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.003090 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 99.88420104980469\n",
      "Avg loss: 0.002847 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 99.86314392089844\n",
      "Avg loss: 0.003249 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 99.88243865966797\n",
      "Avg loss: 0.002877 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 99.88069152832031\n",
      "Avg loss: 0.002911 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 99.87016296386719\n",
      "Avg loss: 0.003120 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 99.87191772460938\n",
      "Avg loss: 0.002887 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 99.87191772460938\n",
      "Avg loss: 0.002904 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 99.87191772460938\n",
      "Avg loss: 0.002915 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 99.87366485595703\n",
      "Avg loss: 0.002981 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.002980 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 99.87366485595703\n",
      "Avg loss: 0.002994 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.002990 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.002990 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.002994 \n",
      "\n",
      "Training took 2.03 minutes.\n",
      "Accuracy: 99.87542724609375\n",
      "Avg loss: 0.002994 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_classical.parameters(), lr=learning_rate_classical)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training for classical\\n--------------------------\")\n",
    "for t in range(epochs_classical):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_classical(train_loader, loss_fn, optimizer)\n",
    "    test_loss = test_loop_classical(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_classical(test_loader, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data for quantum\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 49.04548263549805\n",
      "Avg loss: 0.723206 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 54.41816329956055\n",
      "Avg loss: 0.695978 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 58.6661262512207\n",
      "Avg loss: 0.678617 \n",
      "\n",
      "Training took 36.64 minutes.\n",
      "Accuracy: 58.6661262512207\n",
      "Avg loss: 0.678617 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6786167317063269"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_quantum.parameters(), lr=learning_rate_quantum)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data for quantum\\n--------------------------\")\n",
    "for t in range(epochs_quantum):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_quantum(train_sampled_loader_quantum, loss_fn, optimizer)\n",
    "    test_loss = test_loop_quantum(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_quantum(test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training for quantum\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_quantum):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_loop_quantum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_loop_quantum(test_loader, loss_fn)\n\u001b[1;32m     12\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m, in \u001b[0;36mtrain_loop_quantum\u001b[0;34m(dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m model_quantum(X)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/PC/QML/.venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PC/QML/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PC/QML/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_quantum.parameters(), lr=learning_rate_quantum)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training for quantum\\n--------------------------\")\n",
    "for t in range(epochs_quantum):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_quantum(train_loader, loss_fn, optimizer)\n",
    "    test_loss = test_loop_quantum(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_quantum(test_loader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
