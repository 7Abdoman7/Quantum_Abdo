{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
      "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
      "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
      "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
      "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
      "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
      "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
      "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
      "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "5  0.105915  0.253844  0.081080    3.67      0  \n",
      "6 -0.257237  0.034507  0.005168    4.99      0  \n",
      "7 -0.051634 -1.206921 -1.085339   40.80      0  \n",
      "8 -0.384157  0.011747  0.142404   93.20      0  \n",
      "9  0.094199  0.246219  0.083076    3.68      0  \n",
      "\n",
      "[10 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../../creditcard.csv')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_train(X_train, y_train, sample_size_for_each_class, batch_size):\n",
    "    X_train_sampled = []\n",
    "    y_train_sampled = []\n",
    "\n",
    "    for class_label in np.unique(y_train):\n",
    "        class_indices = np.where(y_train == class_label)[0]\n",
    "        sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class, replace=False)\n",
    "\n",
    "        X_train_sampled.extend(X_train[sampled_indices])\n",
    "        y_train_sampled.extend(y_train[sampled_indices]) \n",
    "\n",
    "    X_train_sampled = np.array(X_train_sampled)\n",
    "    y_train_sampled = np.array(y_train_sampled)\n",
    "\n",
    "    X_train_sampled_torch = torch.tensor(X_train_sampled, dtype=torch.float32)\n",
    "    y_train_sampled_torch = torch.tensor(y_train_sampled, dtype=torch.long)\n",
    "\n",
    "    train_sampled_dataset = TensorDataset(X_train_sampled_torch, y_train_sampled_torch)\n",
    "    train_sampled_loader = DataLoader(train_sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_sampled_loader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer():\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.dev = qml.device('lightning.gpu', wires=self.n_qubits)\n",
    "\n",
    "    def qlayer(self, inputs, weights):\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def circuit(inputs, weights):\n",
    "            qml.AngleEmbedding(inputs, wires=list(np.arange(self.n_qubits)), rotation='X')\n",
    "            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
    "            return tuple(qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits))\n",
    "\n",
    "        return circuit(inputs, weights)  \n",
    "\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers, input_dim, learning_rate=1e-3, batch_size=32, epochs=3):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.quantum_layer = QuantumLayer(n_qubits, n_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, n_qubits)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = nn.Parameter(torch.rand(n_layers, n_qubits, 3) * 1e-3)\n",
    "        self.output_layer = nn.Linear(n_qubits, 2)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(x) * self.scale\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, self.n_qubits), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            expvals = self.quantum_layer.qlayer(x[batch_idx], self.qnn_weights)\n",
    "            out[batch_idx] = torch.stack(expvals)\n",
    "\n",
    "        x = self.output_layer(out)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataloader, loss_fn, optimizer):\n",
    "        self.train()\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = self(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def test_model(self, dataloader, loss_fn):\n",
    "        self.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self(X)\n",
    "                predicted_label = torch.argmax(pred, dim=1)\n",
    "                correct += torch.count_nonzero(predicted_label == y)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        accuracy = (correct / (len(dataloader) * self.batch_size)) * 100\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def train_with_scheduler(self, train_loader, test_loader):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}\\n--------------------------\")\n",
    "            self.train_model(train_loader, loss_fn, optimizer)\n",
    "            test_loss = self.test_model(test_loader, loss_fn)\n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "\n",
    "        self.test_model(test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with full dataset:\n",
      "Epoch 1\n",
      "--------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridClassifier(n_qubits, n_layers, input_dim, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with full dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m sampled_train_loader \u001b[38;5;241m=\u001b[39m sample_from_train(X_train, y_train, sample_size_for_each_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with sampled dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 93\u001b[0m, in \u001b[0;36mHybridClassifier.train_with_scheduler\u001b[0;34m(self, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_model(test_loader, loss_fn)\n\u001b[1;32m     95\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "Cell \u001b[0;32mIn[30], line 60\u001b[0m, in \u001b[0;36mHybridClassifier.train_model\u001b[0;34m(self, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     57\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(X)\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "input_dim = len(X.T)\n",
    "\n",
    "model = HybridClassifier(n_qubits, n_layers, input_dim, batch_size=128)\n",
    "\n",
    "print(\"Training with full dataset:\")\n",
    "model.train_with_scheduler(train_loader, test_loader)\n",
    "\n",
    "sampled_train_loader = sample_from_train(X_train, y_train, sample_size_for_each_class=5, batch_size=32)\n",
    "\n",
    "print(\"\\nTraining with sampled dataset:\")\n",
    "model.train_with_scheduler(sampled_train_loader, test_loader)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\")\n",
    "model.test_model(test_loader, loss_fn=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, learning_rate=1e-3, batch_size=32, epochs=20):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "        self.output_layer = nn.Linear(4, 2)  \n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataloader, loss_fn, optimizer):\n",
    "        self.train()\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = self(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def test_model(self, dataloader, loss_fn):\n",
    "        self.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self(X)\n",
    "                predicted_label = torch.argmax(pred, dim=1)\n",
    "                correct += torch.count_nonzero(predicted_label == y)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        accuracy = (correct / (len(dataloader) * self.batch_size)) * 100\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def train_with_scheduler(self, train_loader, test_loader):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}\\n--------------------------\")\n",
    "            self.train_model(train_loader, loss_fn, optimizer)\n",
    "            test_loss = self.test_model(test_loader, loss_fn)\n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "\n",
    "        self.test_model(test_loader, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classical Model with Full Dataset:\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.003286 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.003244 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 99.87%\n",
      "Avg loss: 0.002997 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002949 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.003171 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.003048 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 99.87%\n",
      "Avg loss: 0.003106 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 99.86%\n",
      "Avg loss: 0.003298 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002998 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 99.87%\n",
      "Avg loss: 0.003066 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002956 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002959 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002906 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002921 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002939 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002984 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.003007 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 99.89%\n",
      "Avg loss: 0.002882 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 99.88%\n",
      "Avg loss: 0.002861 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 99.89%\n",
      "Avg loss: 0.002866 \n",
      "\n",
      "Training took 2.23 minutes.\n",
      "Accuracy: 99.89%\n",
      "Avg loss: 0.002866 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model_classical = ClassicalClassifier(input_dim)\n",
    "\n",
    "print(\"Training Classical Model with Full Dataset:\")\n",
    "model_classical.train_with_scheduler(train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
