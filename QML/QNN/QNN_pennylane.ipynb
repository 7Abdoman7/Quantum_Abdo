{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "y = y[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "\n",
    "map = {'1': 0, '3': 1, '5': 2, '7': 3}\n",
    "y = y.map(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test.to_numpy(),  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 8\n",
    "n_layers = 1\n",
    "\n",
    "dev = qml.device('lightning.gpu', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qlayer(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=list(np.arange(n_qubits)), rotation='X')\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(n_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sampled = []\n",
    "y_train_sampled = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=5, replace=False)\n",
    "\n",
    "    X_train_sampled.extend(X_train[sampled_indices])\n",
    "    y_train_sampled.extend(y_train.iloc[sampled_indices]) \n",
    "\n",
    "X_train_sampled = np.array(X_train_sampled)\n",
    "y_train_sampled = np.array(y_train_sampled)\n",
    "\n",
    "X_train_sampled_torch = torch.tensor(X_train_sampled, dtype=torch.float32)\n",
    "y_train_sampled_torch = torch.tensor(y_train_sampled, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset = TensorDataset(X_train_sampled_torch, y_train_sampled_torch)\n",
    "train_sampled_loader = DataLoader(train_sampled_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 8)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = nn.Parameter(torch.rand(n_layers, n_qubits, 3) * 1e-3)\n",
    "        self.output_layer = nn.Linear(n_qubits, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x)) * self.scale\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, n_qubits), dtype=torch.float32, device=x.device)\n",
    "        for batch_idx in range(batch_size):\n",
    "            expvals = qlayer(x[batch_idx], self.qnn_weights)\n",
    "            out[batch_idx] = torch.stack(expvals)\n",
    "        x = self.output_layer(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 8)\n",
    "        self.output_layer = nn.Linear(8, 4)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    model.to(torch_device)  \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "loss: 1.430017 [   20/   20\n",
      "Accuracy: 27.688547134399414\n",
      "Avg loss: 1.385370 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "loss: 1.312480 [   20/   20\n",
      "Accuracy: 28.282121658325195\n",
      "Avg loss: 1.370455 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "loss: 1.236703 [   20/   20\n",
      "Accuracy: 29.1201114654541\n",
      "Avg loss: 1.358579 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "loss: 1.189448 [   20/   20\n",
      "Accuracy: 30.045391082763672\n",
      "Avg loss: 1.347428 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "loss: 1.151500 [   20/   20\n",
      "Accuracy: 32.01815414428711\n",
      "Avg loss: 1.337312 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "loss: 1.114667 [   20/   20\n",
      "Accuracy: 33.99092102050781\n",
      "Avg loss: 1.328932 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "loss: 1.089090 [   20/   20\n",
      "Accuracy: 35.26536178588867\n",
      "Avg loss: 1.320934 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "loss: 1.066827 [   20/   20\n",
      "Accuracy: 35.701812744140625\n",
      "Avg loss: 1.313004 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "loss: 1.043330 [   20/   20\n",
      "Accuracy: 35.56214904785156\n",
      "Avg loss: 1.305039 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "loss: 1.022351 [   20/   20\n",
      "Accuracy: 35.405029296875\n",
      "Avg loss: 1.297248 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "loss: 1.011688 [   20/   20\n",
      "Accuracy: 35.26536178588867\n",
      "Avg loss: 1.290347 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "loss: 1.005520 [   20/   20\n",
      "Accuracy: 35.387569427490234\n",
      "Avg loss: 1.284936 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "loss: 0.995374 [   20/   20\n",
      "Accuracy: 35.85893630981445\n",
      "Avg loss: 1.281265 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "loss: 0.983720 [   20/   20\n",
      "Accuracy: 36.225555419921875\n",
      "Avg loss: 1.279888 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "loss: 0.972751 [   20/   20\n",
      "Accuracy: 36.69692611694336\n",
      "Avg loss: 1.280205 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "loss: 0.959582 [   20/   20\n",
      "Accuracy: 37.16829299926758\n",
      "Avg loss: 1.280849 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "loss: 0.947008 [   20/   20\n",
      "Accuracy: 37.4127082824707\n",
      "Avg loss: 1.280904 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "loss: 0.938487 [   20/   20\n",
      "Accuracy: 37.831703186035156\n",
      "Avg loss: 1.280242 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "loss: 0.931093 [   20/   20\n",
      "Accuracy: 37.88407897949219\n",
      "Avg loss: 1.278710 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "loss: 0.924241 [   20/   20\n",
      "Accuracy: 38.07611846923828\n",
      "Avg loss: 1.276578 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "loss: 0.919357 [   20/   20\n",
      "Accuracy: 38.32052993774414\n",
      "Avg loss: 1.274606 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "loss: 0.915877 [   20/   20\n",
      "Accuracy: 38.739524841308594\n",
      "Avg loss: 1.273207 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "loss: 0.911577 [   20/   20\n",
      "Accuracy: 39.018856048583984\n",
      "Avg loss: 1.272598 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "loss: 0.907012 [   20/   20\n",
      "Accuracy: 39.19343185424805\n",
      "Avg loss: 1.272665 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "loss: 0.902491 [   20/   20\n",
      "Accuracy: 39.263267517089844\n",
      "Avg loss: 1.272544 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "loss: 0.897753 [   20/   20\n",
      "Accuracy: 39.333099365234375\n",
      "Avg loss: 1.271823 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "loss: 0.893300 [   20/   20\n",
      "Accuracy: 39.52513885498047\n",
      "Avg loss: 1.270584 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "loss: 0.889273 [   20/   20\n",
      "Accuracy: 39.73463439941406\n",
      "Avg loss: 1.269304 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "loss: 0.885189 [   20/   20\n",
      "Accuracy: 39.94413375854492\n",
      "Avg loss: 1.268377 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "loss: 0.880811 [   20/   20\n",
      "Accuracy: 39.96158981323242\n",
      "Avg loss: 1.267754 \n",
      "\n",
      "Training took 41.59 minutes.\n",
      "Accuracy: 39.96158981323242\n",
      "Avg loss: 1.267754 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data\\n--------------------------\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_sampled_loader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_loader, model, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "loss: 1.108349 [   32/22899\n",
      "loss: 0.171429 [ 3232/22899\n",
      "loss: 0.079054 [ 6432/22899\n",
      "loss: 0.031366 [ 9632/22899\n",
      "loss: 0.020665 [12832/22899\n",
      "loss: 0.150173 [16032/22899\n",
      "loss: 0.128595 [19232/22899\n",
      "loss: 0.007454 [22432/22899\n",
      "Accuracy: 97.46857452392578\n",
      "Avg loss: 0.106847 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "loss: 0.134964 [   32/22899\n",
      "loss: 0.012960 [ 3232/22899\n",
      "loss: 0.097291 [ 6432/22899\n",
      "loss: 0.006723 [ 9632/22899\n",
      "loss: 0.002523 [12832/22899\n",
      "loss: 0.105229 [16032/22899\n",
      "loss: 0.208252 [19232/22899\n",
      "loss: 0.009216 [22432/22899\n",
      "Accuracy: 98.09706115722656\n",
      "Avg loss: 0.086666 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "loss: 0.015706 [   32/22899\n",
      "loss: 0.008369 [ 3232/22899\n",
      "loss: 0.008434 [ 6432/22899\n",
      "loss: 0.002029 [ 9632/22899\n",
      "loss: 0.000647 [12832/22899\n",
      "loss: 0.026236 [16032/22899\n",
      "loss: 0.013096 [19232/22899\n",
      "loss: 0.002438 [22432/22899\n",
      "Accuracy: 98.1668930053711\n",
      "Avg loss: 0.126141 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "loss: 0.002677 [   32/22899\n",
      "loss: 0.000736 [ 3232/22899\n",
      "loss: 0.007020 [ 6432/22899\n",
      "loss: 0.019178 [ 9632/22899\n",
      "loss: 0.007573 [12832/22899\n",
      "loss: 0.102318 [16032/22899\n",
      "loss: 0.024247 [19232/22899\n",
      "loss: 0.012105 [22432/22899\n",
      "Accuracy: 98.27164459228516\n",
      "Avg loss: 0.121580 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "loss: 0.000209 [   32/22899\n",
      "loss: 0.008245 [ 3232/22899\n",
      "loss: 0.053731 [ 6432/22899\n",
      "loss: 0.097091 [ 9632/22899\n",
      "loss: 0.018359 [12832/22899\n",
      "loss: 0.140834 [16032/22899\n",
      "loss: 0.026924 [19232/22899\n",
      "loss: 0.000649 [22432/22899\n",
      "Accuracy: 98.48114013671875\n",
      "Avg loss: 0.121569 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "loss: 0.005539 [   32/22899\n",
      "loss: 0.099363 [ 3232/22899\n",
      "loss: 0.000417 [ 6432/22899\n",
      "loss: 0.000764 [ 9632/22899\n",
      "loss: 0.002559 [12832/22899\n",
      "loss: 0.002033 [16032/22899\n",
      "loss: 0.013184 [19232/22899\n",
      "loss: 0.010265 [22432/22899\n",
      "Accuracy: 97.83519744873047\n",
      "Avg loss: 0.135170 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "loss: 0.013348 [   32/22899\n",
      "loss: 0.003283 [ 3232/22899\n",
      "loss: 0.001610 [ 6432/22899\n",
      "loss: 0.002786 [ 9632/22899\n",
      "loss: 0.002260 [12832/22899\n",
      "loss: 0.001530 [16032/22899\n",
      "loss: 0.000218 [19232/22899\n",
      "loss: 0.006681 [22432/22899\n",
      "Accuracy: 98.30656433105469\n",
      "Avg loss: 0.144842 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "loss: 0.002477 [   32/22899\n",
      "loss: 0.000040 [ 3232/22899\n",
      "loss: 0.000226 [ 6432/22899\n",
      "loss: 0.001198 [ 9632/22899\n",
      "loss: 0.031037 [12832/22899\n",
      "loss: 0.001703 [16032/22899\n",
      "loss: 0.044671 [19232/22899\n",
      "loss: 0.000208 [22432/22899\n",
      "Accuracy: 98.20181274414062\n",
      "Avg loss: 0.181306 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "loss: 0.091456 [   32/22899\n",
      "loss: 0.067929 [ 3232/22899\n",
      "loss: 0.002613 [ 6432/22899\n",
      "loss: 0.000135 [ 9632/22899\n",
      "loss: 0.005962 [12832/22899\n",
      "loss: 0.001558 [16032/22899\n",
      "loss: 0.004795 [19232/22899\n",
      "loss: 0.019967 [22432/22899\n",
      "Accuracy: 98.62081146240234\n",
      "Avg loss: 0.115587 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "loss: 0.000500 [   32/22899\n",
      "loss: 0.000870 [ 3232/22899\n",
      "loss: 0.008450 [ 6432/22899\n",
      "loss: 0.000245 [ 9632/22899\n",
      "loss: 0.000083 [12832/22899\n",
      "loss: 0.000028 [16032/22899\n",
      "loss: 0.000585 [19232/22899\n",
      "loss: 0.000450 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.116757 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "loss: 0.000517 [   32/22899\n",
      "loss: 0.000547 [ 3232/22899\n",
      "loss: 0.002150 [ 6432/22899\n",
      "loss: 0.000005 [ 9632/22899\n",
      "loss: 0.000037 [12832/22899\n",
      "loss: 0.000017 [16032/22899\n",
      "loss: 0.000952 [19232/22899\n",
      "loss: 0.001191 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.122899 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "loss: 0.005407 [   32/22899\n",
      "loss: 0.000726 [ 3232/22899\n",
      "loss: 0.000002 [ 6432/22899\n",
      "loss: 0.004023 [ 9632/22899\n",
      "loss: 0.000212 [12832/22899\n",
      "loss: 0.000458 [16032/22899\n",
      "loss: 0.000006 [19232/22899\n",
      "loss: 0.000023 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.129766 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "loss: 0.000034 [   32/22899\n",
      "loss: 0.000138 [ 3232/22899\n",
      "loss: 0.000201 [ 6432/22899\n",
      "loss: 0.000719 [ 9632/22899\n",
      "loss: 0.000083 [12832/22899\n",
      "loss: 0.000076 [16032/22899\n",
      "loss: 0.000757 [19232/22899\n",
      "loss: 0.000032 [22432/22899\n",
      "Accuracy: 98.70809936523438\n",
      "Avg loss: 0.138178 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "loss: 0.000007 [   32/22899\n",
      "loss: 0.000187 [ 3232/22899\n",
      "loss: 0.000056 [ 6432/22899\n",
      "loss: 0.000078 [ 9632/22899\n",
      "loss: 0.000154 [12832/22899\n",
      "loss: 0.000002 [16032/22899\n",
      "loss: 0.000007 [19232/22899\n",
      "loss: 0.000001 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.144114 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "loss: 0.001189 [   32/22899\n",
      "loss: 0.000010 [ 3232/22899\n",
      "loss: 0.000010 [ 6432/22899\n",
      "loss: 0.000002 [ 9632/22899\n",
      "loss: 0.000048 [12832/22899\n",
      "loss: 0.000005 [16032/22899\n",
      "loss: 0.000004 [19232/22899\n",
      "loss: 0.000012 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.145164 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "loss: 0.000026 [   32/22899\n",
      "loss: 0.000026 [ 3232/22899\n",
      "loss: 0.000127 [ 6432/22899\n",
      "loss: 0.000001 [ 9632/22899\n",
      "loss: 0.000077 [12832/22899\n",
      "loss: 0.000001 [16032/22899\n",
      "loss: 0.000119 [19232/22899\n",
      "loss: 0.000001 [22432/22899\n",
      "Accuracy: 98.67317962646484\n",
      "Avg loss: 0.146443 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "loss: 0.000023 [   32/22899\n",
      "loss: 0.016947 [ 3232/22899\n",
      "loss: 0.000398 [ 6432/22899\n",
      "loss: 0.000008 [ 9632/22899\n",
      "loss: 0.053048 [12832/22899\n",
      "loss: 0.000003 [16032/22899\n",
      "loss: 0.000007 [19232/22899\n",
      "loss: 0.000006 [22432/22899\n",
      "Accuracy: 98.69064331054688\n",
      "Avg loss: 0.148097 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "loss: 0.000002 [   32/22899\n",
      "loss: 0.000025 [ 3232/22899\n",
      "loss: 0.000021 [ 6432/22899\n",
      "loss: 0.000003 [ 9632/22899\n",
      "loss: 0.000005 [12832/22899\n",
      "loss: 0.000003 [16032/22899\n",
      "loss: 0.000004 [19232/22899\n",
      "loss: 0.000025 [22432/22899\n",
      "Accuracy: 98.67317962646484\n",
      "Avg loss: 0.150116 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "loss: 0.000509 [   32/22899\n",
      "loss: 0.000005 [ 3232/22899\n",
      "loss: 0.000083 [ 6432/22899\n",
      "loss: 0.000000 [ 9632/22899\n",
      "loss: 0.000000 [12832/22899\n",
      "loss: 0.000001 [16032/22899\n",
      "loss: 0.000014 [19232/22899\n",
      "loss: 0.000086 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.152585 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "loss: 0.000010 [   32/22899\n",
      "loss: 0.000447 [ 3232/22899\n",
      "loss: 0.000019 [ 6432/22899\n",
      "loss: 0.000000 [ 9632/22899\n",
      "loss: 0.000001 [12832/22899\n",
      "loss: 0.000330 [16032/22899\n",
      "loss: 0.000025 [19232/22899\n",
      "loss: 0.000006 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.155090 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "loss: 0.000001 [   32/22899\n",
      "loss: 0.000003 [ 3232/22899\n",
      "loss: 0.000001 [ 6432/22899\n",
      "loss: 0.000000 [ 9632/22899\n",
      "loss: 0.000047 [12832/22899\n",
      "loss: 0.000031 [16032/22899\n",
      "loss: 0.000000 [19232/22899\n",
      "loss: 0.000001 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.155349 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "loss: 0.000029 [   32/22899\n",
      "loss: 0.000007 [ 3232/22899\n",
      "loss: 0.000356 [ 6432/22899\n",
      "loss: 0.000001 [ 9632/22899\n",
      "loss: 0.000002 [12832/22899\n",
      "loss: 0.000001 [16032/22899\n",
      "loss: 0.000006 [19232/22899\n",
      "loss: 0.000006 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.155682 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "loss: 0.000001 [   32/22899\n",
      "loss: 0.000079 [ 3232/22899\n",
      "loss: 0.000007 [ 6432/22899\n",
      "loss: 0.000001 [ 9632/22899\n",
      "loss: 0.000122 [12832/22899\n",
      "loss: 0.000000 [16032/22899\n",
      "loss: 0.000007 [19232/22899\n",
      "loss: 0.000340 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.156037 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "loss: 0.000000 [   32/22899\n",
      "loss: 0.000253 [ 3232/22899\n",
      "loss: 0.000000 [ 6432/22899\n",
      "loss: 0.023178 [ 9632/22899\n",
      "loss: 0.000344 [12832/22899\n",
      "loss: 0.000003 [16032/22899\n",
      "loss: 0.000004 [19232/22899\n",
      "loss: 0.000267 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.156432 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "loss: 0.000093 [   32/22899\n",
      "loss: 0.000000 [ 3232/22899\n",
      "loss: 0.000000 [ 6432/22899\n",
      "loss: 0.000001 [ 9632/22899\n",
      "loss: 0.000004 [12832/22899\n",
      "loss: 0.000012 [16032/22899\n",
      "loss: 0.000003 [19232/22899\n",
      "loss: 0.000003 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.156867 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "loss: 0.000015 [   32/22899\n",
      "loss: 0.000000 [ 3232/22899\n",
      "loss: 0.000049 [ 6432/22899\n",
      "loss: 0.000009 [ 9632/22899\n",
      "loss: 0.000002 [12832/22899\n",
      "loss: 0.000002 [16032/22899\n",
      "loss: 0.000002 [19232/22899\n",
      "loss: 0.000316 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157252 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "loss: 0.000005 [   32/22899\n",
      "loss: 0.000984 [ 3232/22899\n",
      "loss: 0.000001 [ 6432/22899\n",
      "loss: 0.000007 [ 9632/22899\n",
      "loss: 0.000008 [12832/22899\n",
      "loss: 0.000028 [16032/22899\n",
      "loss: 0.000000 [19232/22899\n",
      "loss: 0.000019 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157294 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "loss: 0.000001 [   32/22899\n",
      "loss: 0.000000 [ 3232/22899\n",
      "loss: 0.000002 [ 6432/22899\n",
      "loss: 0.000095 [ 9632/22899\n",
      "loss: 0.000050 [12832/22899\n",
      "loss: 0.000014 [16032/22899\n",
      "loss: 0.000004 [19232/22899\n",
      "loss: 0.000146 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157334 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "loss: 0.000003 [   32/22899\n",
      "loss: 0.000002 [ 3232/22899\n",
      "loss: 0.000014 [ 6432/22899\n",
      "loss: 0.000157 [ 9632/22899\n",
      "loss: 0.000002 [12832/22899\n",
      "loss: 0.000124 [16032/22899\n",
      "loss: 0.000137 [19232/22899\n",
      "loss: 0.000003 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157379 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "loss: 0.000031 [   32/22899\n",
      "loss: 0.000001 [ 3232/22899\n",
      "loss: 0.000003 [ 6432/22899\n",
      "loss: 0.000017 [ 9632/22899\n",
      "loss: 0.000004 [12832/22899\n",
      "loss: 0.000008 [16032/22899\n",
      "loss: 0.000002 [19232/22899\n",
      "loss: 0.000282 [22432/22899\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157423 \n",
      "\n",
      "Training took 0.55 minutes.\n",
      "Accuracy: 98.65572357177734\n",
      "Avg loss: 0.157423 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training\\n--------------------------\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_loader, model, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
