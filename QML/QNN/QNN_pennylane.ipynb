{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "y = y[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "\n",
    "map = {'1': 0, '3': 1, '5': 2, '7': 3}\n",
    "y = y.map(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test.to_numpy(),  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 8\n",
    "n_layers = 1\n",
    "\n",
    "dev = qml.device('lightning.gpu', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qlayer(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=list(np.arange(n_qubits)), rotation='X')\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(n_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 128)\n",
    "        self.fc2 = nn.Linear(128, n_qubits)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = nn.Parameter(torch.rand(n_layers, n_qubits, 3) * 1e-3)\n",
    "        self.output_layer = nn.Linear(n_qubits, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(x) * self.scale\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, n_qubits), dtype=torch.float32, device=x.device)\n",
    "        for batch_idx in range(batch_size):\n",
    "            expvals = qlayer(x[batch_idx], self.qnn_weights)\n",
    "            out[batch_idx] = torch.stack(expvals)\n",
    "        x = self.output_layer(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 128)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "        self.output_layer = nn.Linear(8, 4)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_for_each_class_classical = 5\n",
    "batch_size_classical = 32\n",
    "\n",
    "X_train_sampled_classical = []\n",
    "y_train_sampled_classical = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class_classical, replace=False)\n",
    "\n",
    "    X_train_sampled_classical.extend(X_train[sampled_indices])\n",
    "    y_train_sampled_classical.extend(y_train.iloc[sampled_indices]) \n",
    "\n",
    "X_train_sampled_classical = np.array(X_train_sampled_classical)\n",
    "y_train_sampled_classical = np.array(y_train_sampled_classical)\n",
    "\n",
    "X_train_sampled_torch_classical = torch.tensor(X_train_sampled_classical, dtype=torch.float32)\n",
    "y_train_sampled_torch_classical = torch.tensor(y_train_sampled_classical, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset_classical = TensorDataset(X_train_sampled_torch_classical, y_train_sampled_torch_classical)\n",
    "train_sampled_loader_classical = DataLoader(train_sampled_dataset_classical, batch_size=batch_size_classical, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_for_each_class_quantum = 5\n",
    "batch_size_quantum = 32\n",
    "\n",
    "X_train_sampled_quantum = []\n",
    "y_train_sampled_quantum = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class_quantum, replace=False)\n",
    "\n",
    "    X_train_sampled_quantum.extend(X_train[sampled_indices])\n",
    "    y_train_sampled_quantum.extend(y_train.iloc[sampled_indices]) \n",
    "\n",
    "X_train_sampled_quantum = np.array(X_train_sampled_quantum)\n",
    "y_train_sampled_quantum = np.array(y_train_sampled_quantum)\n",
    "\n",
    "X_train_sampled_torch_quantum = torch.tensor(X_train_sampled_quantum, dtype=torch.float32)\n",
    "y_train_sampled_torch_quantum = torch.tensor(y_train_sampled_quantum, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset_quantum = TensorDataset(X_train_sampled_torch_quantum, y_train_sampled_torch_quantum)\n",
    "train_sampled_loader_quantum = DataLoader(train_sampled_dataset_quantum, batch_size=batch_size_quantum, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantum = HybridClassifier()\n",
    "\n",
    "learning_rate_quantum = 1e-3\n",
    "batch_size_quantum = 32\n",
    "epochs_quantum = 3\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop_quantum(dataloader, loss_fn, optimizer):\n",
    "    model_quantum.train()\n",
    "    model_quantum.to(torch_device)  \n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model_quantum(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def test_loop_quantum(dataloader, loss_fn):\n",
    "    model_quantum.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model_quantum.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model_quantum(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size_quantum) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classical = ClassicalClassifier()\n",
    "\n",
    "learning_rate_classical = 1e-3\n",
    "batch_size_classical = 32\n",
    "epochs_classical = 20\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop_classical(dataloader, loss_fn, optimizer):\n",
    "    model_classical.train()\n",
    "    model_classical.to(torch_device)  \n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model_classical(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def test_loop_classical(dataloader, loss_fn):\n",
    "    model_classical.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model_classical.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model_classical(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size_classical) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data for classical\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 22.014663696289062\n",
      "Avg loss: 1.395014 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 22.206703186035156\n",
      "Avg loss: 1.373089 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 22.74790382385254\n",
      "Avg loss: 1.349440 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 24.07472038269043\n",
      "Avg loss: 1.324016 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 26.204607009887695\n",
      "Avg loss: 1.297568 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 30.499300003051758\n",
      "Avg loss: 1.270469 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 38.32052993774414\n",
      "Avg loss: 1.242235 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 48.411312103271484\n",
      "Avg loss: 1.213175 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 59.96857452392578\n",
      "Avg loss: 1.183759 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 66.81214904785156\n",
      "Avg loss: 1.154113 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 70.14664459228516\n",
      "Avg loss: 1.124366 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 71.8575439453125\n",
      "Avg loss: 1.094496 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 72.93994140625\n",
      "Avg loss: 1.064395 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 73.62081146240234\n",
      "Avg loss: 1.034893 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 73.86521911621094\n",
      "Avg loss: 1.006227 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 74.07472229003906\n",
      "Avg loss: 0.978534 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 74.33658599853516\n",
      "Avg loss: 0.952087 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 74.35404968261719\n",
      "Avg loss: 0.927608 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 74.51116943359375\n",
      "Avg loss: 0.904595 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 74.68575286865234\n",
      "Avg loss: 0.882307 \n",
      "\n",
      "Training took 0.02 minutes.\n",
      "Accuracy: 74.68575286865234\n",
      "Avg loss: 0.882307 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8823071035592915"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_classical.parameters(), lr=learning_rate_classical)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data for classical\\n--------------------------\")\n",
    "for t in range(epochs_classical):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_classical(train_sampled_loader_classical, loss_fn, optimizer)\n",
    "    test_loss = test_loop_classical(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_classical(test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training for classical\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 97.76536560058594\n",
      "Avg loss: 0.086503 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 98.06214904785156\n",
      "Avg loss: 0.086084 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 98.30656433105469\n",
      "Avg loss: 0.080448 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 98.34147644042969\n",
      "Avg loss: 0.090645 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 98.62081146240234\n",
      "Avg loss: 0.088699 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 98.42876434326172\n",
      "Avg loss: 0.122780 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 98.49860382080078\n",
      "Avg loss: 0.135819 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 98.32402038574219\n",
      "Avg loss: 0.155733 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 98.39385223388672\n",
      "Avg loss: 0.185733 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 98.56843566894531\n",
      "Avg loss: 0.174088 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 98.62081146240234\n",
      "Avg loss: 0.171816 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 98.62081146240234\n",
      "Avg loss: 0.169996 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 98.60334777832031\n",
      "Avg loss: 0.169403 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 98.56843566894531\n",
      "Avg loss: 0.168874 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 98.58589172363281\n",
      "Avg loss: 0.165722 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 98.58589172363281\n",
      "Avg loss: 0.166471 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 98.56843566894531\n",
      "Avg loss: 0.166904 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 98.55097198486328\n",
      "Avg loss: 0.166981 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 98.55097198486328\n",
      "Avg loss: 0.167069 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 98.55097198486328\n",
      "Avg loss: 0.167083 \n",
      "\n",
      "Training took 0.30 minutes.\n",
      "Accuracy: 98.55097198486328\n",
      "Avg loss: 0.167083 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_classical.parameters(), lr=learning_rate_classical)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training for classical\\n--------------------------\")\n",
    "for t in range(epochs_classical):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_classical(train_loader, loss_fn, optimizer)\n",
    "    test_loss = test_loop_classical(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_classical(test_loader, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data for quantum\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 29.975557327270508\n",
      "Avg loss: 1.319976 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 31.023042678833008\n",
      "Avg loss: 1.280532 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 31.70391082763672\n",
      "Avg loss: 1.263052 \n",
      "\n",
      "Training took 3.70 minutes.\n",
      "Accuracy: 31.70391082763672\n",
      "Avg loss: 1.263052 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.263051777578599"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_quantum.parameters(), lr=learning_rate_quantum)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data for quantum\\n--------------------------\")\n",
    "for t in range(epochs_quantum):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_quantum(train_sampled_loader_quantum, loss_fn, optimizer)\n",
    "    test_loss = test_loop_quantum(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_quantum(test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training for quantum\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 94.32611083984375\n",
      "Avg loss: 0.247705 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 96.45600128173828\n",
      "Avg loss: 0.153005 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 97.03211975097656\n",
      "Avg loss: 0.129944 \n",
      "\n",
      "Training took 41.44 minutes.\n",
      "Accuracy: 97.03211975097656\n",
      "Avg loss: 0.129944 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1299443712364362"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_quantum.parameters(), lr=learning_rate_quantum)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training for quantum\\n--------------------------\")\n",
    "for t in range(epochs_quantum):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop_quantum(train_loader, loss_fn, optimizer)\n",
    "    test_loss = test_loop_quantum(test_loader, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop_quantum(test_loader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
