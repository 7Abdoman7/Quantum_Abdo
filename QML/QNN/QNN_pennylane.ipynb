{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "y = y[(y == '1') | (y == '3') | (y == '5') | (y == '7')]\n",
    "\n",
    "map = {'1': 0, '3': 1, '5': 2, '7': 3}\n",
    "y = y.map(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi/2))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test.to_numpy(),  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_train(X_train, y_train, sample_size_for_each_class, batch_size):\n",
    "    X_train_sampled = []\n",
    "    y_train_sampled = []\n",
    "\n",
    "    y_train_np = y_train.to_numpy()\n",
    "\n",
    "    for class_label in np.unique(y_train_np):\n",
    "        class_indices = np.where(y_train_np == class_label)[0]\n",
    "        sampled_indices = np.random.choice(class_indices, size=sample_size_for_each_class, replace=False)\n",
    "\n",
    "        X_train_sampled.extend(X_train[sampled_indices])\n",
    "        y_train_sampled.extend(y_train_np[sampled_indices])\n",
    "\n",
    "    X_train_sampled = np.array(X_train_sampled)\n",
    "    y_train_sampled = np.array(y_train_sampled)\n",
    "\n",
    "    X_train_sampled_torch = torch.tensor(X_train_sampled, dtype=torch.float32)\n",
    "    y_train_sampled_torch = torch.tensor(y_train_sampled, dtype=torch.long)\n",
    "\n",
    "    train_sampled_dataset = TensorDataset(X_train_sampled_torch, y_train_sampled_torch)\n",
    "    train_sampled_loader = DataLoader(train_sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_sampled_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer():\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.dev = qml.device('lightning.gpu', wires=self.n_qubits)\n",
    "\n",
    "    def qlayer(self, inputs, weights):\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def circuit(inputs, weights):\n",
    "            qml.AngleEmbedding(inputs, wires=list(np.arange(self.n_qubits)), rotation='X')\n",
    "            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i) @ qml.PauliZ((i+1)%self.n_qubits)) for i in range(self.n_qubits)] \n",
    "\n",
    "        return circuit(inputs, weights)  \n",
    "\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers, input_dim, learning_rate=1e-3, batch_size=32, epochs=3):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.quantum_layer = QuantumLayer(n_qubits, n_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = nn.Parameter(torch.rand(n_layers, n_qubits, 3) * np.pi)\n",
    "        self.output_layer = nn.Linear(n_qubits, 4)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.tanh(x) * self.scale\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, self.n_qubits), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            expvals = self.quantum_layer.qlayer(x[batch_idx], self.qnn_weights)\n",
    "            out[batch_idx] = torch.stack(expvals)\n",
    "\n",
    "        x = self.output_layer(out)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataloader, loss_fn, optimizer):\n",
    "        self.train()\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = self(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def test_model(self, dataloader, loss_fn):\n",
    "        self.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self(X)\n",
    "                predicted_label = torch.argmax(pred, dim=1)\n",
    "                correct += torch.count_nonzero(predicted_label == y)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        accuracy = (correct / (len(dataloader) * self.batch_size)) * 100\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def train_with_scheduler(self, train_loader):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9, nesterov=True)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "        train_indices = np.arange(len(train_loader.dataset))\n",
    "        np.random.shuffle(train_indices)\n",
    "        val_size = int(0.1 * len(train_indices))  \n",
    "        val_indices, train_indices = train_indices[:val_size], train_indices[val_size:]\n",
    "\n",
    "        val_subset = Subset(train_loader.dataset, val_indices)\n",
    "        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        start_time = time.time()\n",
    " \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}\\n--------------------------\")\n",
    "            self.train_model(train_loader, loss_fn, optimizer)\n",
    "            val_loss = self.test_model(val_loader, loss_fn)  \n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training took {elapsed_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, learning_rate=1e-3, batch_size=32, epochs=20):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.output_layer = nn.Linear(8, 4)  \n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataloader, loss_fn, optimizer):\n",
    "        self.train()\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = self(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def test_model(self, dataloader, loss_fn):\n",
    "        self.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self(X)\n",
    "                predicted_label = torch.argmax(pred, dim=1)\n",
    "                correct += torch.count_nonzero(predicted_label == y)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        accuracy = (correct / (len(dataloader) * self.batch_size)) * 100\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        return test_loss, accuracy\n",
    "\n",
    "    def train_with_scheduler(self, train_loader):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "        train_indices = np.arange(len(train_loader.dataset))\n",
    "        np.random.shuffle(train_indices)\n",
    "        val_size = int(0.1 * len(train_indices))  \n",
    "        val_indices, train_indices = train_indices[:val_size], train_indices[val_size:]\n",
    "\n",
    "        val_subset = Subset(train_loader.dataset, val_indices)\n",
    "        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        start_time = time.time()\n",
    " \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}\\n--------------------------\")\n",
    "            self.train_model(train_loader, loss_fn, optimizer)\n",
    "            val_loss = self.test_model(val_loader, loss_fn)  \n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training took {elapsed_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_loader = sample_from_train(X_train, y_train, sample_size_for_each_class=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightningException",
     "evalue": "[/project/pennylane_lightning/core/src/utils/cuda_utils/DevicePool.hpp][Line:86][Method:getTotalDevices]: Error in PennyLane Lightning: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightningException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      3\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHybridClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with sampled dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_with_scheduler(sampled_train_loader)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mHybridClassifier.__init__\u001b[0;34m(self, n_qubits, n_layers, input_dim, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantum_layer \u001b[38;5;241m=\u001b[39m \u001b[43mQuantumLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mQuantumLayer.__init__\u001b[0;34m(self, n_qubits, n_layers)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_qubits \u001b[38;5;241m=\u001b[39m n_qubits\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m=\u001b[39m n_layers\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlightning.gpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_qubits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/pennylane/devices/device_constructor.py:263\u001b[0m, in \u001b[0;36mdevice\u001b[0;34m(name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mDeviceError(\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m plugin requires PennyLane versions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_versions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhowever PennyLane version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqml\u001b[38;5;241m.\u001b[39mversion()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m         )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Construct the device\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mplugin_device_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Once the device is constructed, we set its custom expansion function if\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# any custom decompositions were specified.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_decomps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/pennylane_lightning/lightning_gpu/lightning_gpu.py:256\u001b[0m, in \u001b[0;36mLightningGPU.__init__\u001b[0;34m(self, wires, c_dtype, shots, batch_obs, mpi, mpi_buf_size, use_async)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_CPP_BINARY_AVAILABLE:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-compiled binaries for lightning.gpu are not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo manually compile from source, follow the instructions at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.pennylane.ai/projects/lightning/en/stable/dev/installation.html.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n\u001b[0;32m--> 256\u001b[0m \u001b[43mcheck_gpu_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    259\u001b[0m     wires\u001b[38;5;241m=\u001b[39mwires,\n\u001b[1;32m    260\u001b[0m     c_dtype\u001b[38;5;241m=\u001b[39mc_dtype,\n\u001b[1;32m    261\u001b[0m     shots\u001b[38;5;241m=\u001b[39mshots,\n\u001b[1;32m    262\u001b[0m     batch_obs\u001b[38;5;241m=\u001b[39mbatch_obs,\n\u001b[1;32m    263\u001b[0m )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Set the attributes to call the LightningGPU classes\u001b[39;00m\n",
      "File \u001b[0;32m~/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/pennylane_lightning/lightning_gpu/lightning_gpu.py:189\u001b[0m, in \u001b[0;36mcheck_gpu_resources\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m find_library(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustatevec\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m imp_util\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuquantum\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuStateVec libraries not found. Please pip install the appropriate cuStateVec library in a virtual environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m     )\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mDevPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTotalDevices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported CUDA-capable device found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_gpu_supported():\n",
      "\u001b[0;31mLightningException\u001b[0m: [/project/pennylane_lightning/core/src/utils/cuda_utils/DevicePool.hpp][Line:86][Method:getTotalDevices]: Error in PennyLane Lightning: unknown error"
     ]
    }
   ],
   "source": [
    "n_qubits = 8\n",
    "n_layers = 4\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = HybridClassifier(n_qubits, n_layers, input_dim, batch_size=32, epochs=20, learning_rate=0.01)\n",
    "\n",
    "print(\"\\nTraining with sampled dataset:\")\n",
    "model.train_with_scheduler(sampled_train_loader)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\")\n",
    "model.test_model(test_loader, loss_fn=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahmanelsayed/PC/Quantum_Abdo/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classical Model with sample Dataset:\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 21.88%\n",
      "Avg loss: 1.122073 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 21.88%\n",
      "Avg loss: 0.883775 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.531198 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 18.75%\n",
      "Avg loss: 0.251568 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.142732 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.003685 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.000110 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.000579 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.000634 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002928 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.045063 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.004634 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.003338 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.003201 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.003047 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002885 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002739 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002599 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002467 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002454 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002440 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002426 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002411 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002397 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002383 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002382 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002380 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002379 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002378 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "Accuracy: 25.00%\n",
      "Avg loss: 0.002376 \n",
      "\n",
      "Training took 0.00 minutes.\n",
      "\n",
      "Evaluating on test set:\n",
      "Accuracy: 87.50%\n",
      "Avg loss: 1.026540 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0265402381467268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model_classical = ClassicalClassifier(input_dim, batch_size=32, epochs=30, learning_rate=0.01)\n",
    "\n",
    "\n",
    "print(\"Training Classical Model with sample Dataset:\")\n",
    "model_classical.train_with_scheduler(sampled_train_loader)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\")\n",
    "model_classical.test_model(test_loader, loss_fn=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classical Model with Full Dataset:\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Accuracy: 93.12%\n",
      "Avg loss: 0.244655 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Accuracy: 98.44%\n",
      "Avg loss: 0.054106 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Accuracy: 98.75%\n",
      "Avg loss: 0.040742 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Accuracy: 98.96%\n",
      "Avg loss: 0.044608 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Accuracy: 98.59%\n",
      "Avg loss: 0.048365 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Accuracy: 99.06%\n",
      "Avg loss: 0.029403 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Accuracy: 98.65%\n",
      "Avg loss: 0.036461 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Accuracy: 99.22%\n",
      "Avg loss: 0.020570 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Accuracy: 98.39%\n",
      "Avg loss: 0.053110 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Accuracy: 98.91%\n",
      "Avg loss: 0.096479 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Accuracy: 99.48%\n",
      "Avg loss: 0.014507 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Accuracy: 99.48%\n",
      "Avg loss: 0.023573 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Accuracy: 97.03%\n",
      "Avg loss: 0.166527 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Accuracy: 97.97%\n",
      "Avg loss: 0.076805 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Accuracy: 98.59%\n",
      "Avg loss: 0.075617 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Accuracy: 99.32%\n",
      "Avg loss: 0.024778 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Accuracy: 99.22%\n",
      "Avg loss: 0.020238 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Accuracy: 99.64%\n",
      "Avg loss: 0.013892 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Accuracy: 99.69%\n",
      "Avg loss: 0.009843 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Accuracy: 99.69%\n",
      "Avg loss: 0.008565 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "Accuracy: 99.69%\n",
      "Avg loss: 0.006359 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "Accuracy: 99.74%\n",
      "Avg loss: 0.006386 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "Accuracy: 99.74%\n",
      "Avg loss: 0.005724 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "Accuracy: 99.74%\n",
      "Avg loss: 0.004296 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "Accuracy: 99.79%\n",
      "Avg loss: 0.003178 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "Accuracy: 99.79%\n",
      "Avg loss: 0.003082 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "Accuracy: 99.79%\n",
      "Avg loss: 0.002714 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "Accuracy: 99.79%\n",
      "Avg loss: 0.002989 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "Accuracy: 99.74%\n",
      "Avg loss: 0.006155 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "Accuracy: 99.79%\n",
      "Avg loss: 0.002503 \n",
      "\n",
      "Training took 1.46 minutes.\n",
      "\n",
      "Evaluating on test set:\n",
      "Accuracy: 98.55%\n",
      "Avg loss: 0.131544 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13154431012679194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model_classical = ClassicalClassifier(input_dim, batch_size=32, epochs=30, learning_rate=0.01)\n",
    "\n",
    "\n",
    "print(\"Training Classical Model with Full Dataset:\")\n",
    "model_classical.train_with_scheduler(train_loader)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\")\n",
    "model_classical.test_model(test_loader, loss_fn=nn.CrossEntropyLoss())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
