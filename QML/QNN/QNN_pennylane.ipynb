{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_torch  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_torch  = torch.tensor(y_test.to_numpy(),  dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset  = TensorDataset(X_test_torch,  y_test_torch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 16\n",
    "n_layers = 1\n",
    "\n",
    "dev = qml.device('lightning.gpu', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qlayer(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=list(np.arange(n_qubits)), rotation='X')\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(n_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sampled = []\n",
    "y_train_sampled = []\n",
    "\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=50, replace=False)\n",
    "\n",
    "    X_train_sampled.extend(X_train[sampled_indices])\n",
    "    y_train_sampled.extend(y_train.iloc[sampled_indices]) \n",
    "\n",
    "X_train_sampled = np.array(X_train_sampled)\n",
    "y_train_sampled = np.array(y_train_sampled)\n",
    "\n",
    "X_train_sampled_torch = torch.tensor(X_train_sampled, dtype=torch.float32)\n",
    "y_train_sampled_torch = torch.tensor(y_train_sampled, dtype=torch.long)\n",
    "\n",
    "train_sampled_dataset = TensorDataset(X_train_sampled_torch, y_train_sampled_torch)\n",
    "train_sampled_loader = DataLoader(train_sampled_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.scale = nn.Parameter(torch.tensor([2 * np.pi]))\n",
    "        self.qnn_weights = torch.rand(n_layers, n_qubits, 3) * 1e-3\n",
    "        self.output_layer = nn.Linear(n_qubits, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x)) * self.scale\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, n_qubits), dtype=torch.float32, device=x.device) \n",
    "        for batch_index in range(batch_size):\n",
    "            expval_tensors = qlayer(x[batch_index], self.qnn_weights)\n",
    "            expval_floats = [t.item() for t in expval_tensors]\n",
    "            out[batch_index] = torch.tensor(expval_floats, device=x.device)\n",
    "        x = self.output_layer(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(X.T), 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.output_layer = nn.Linear(16, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassicalClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    model.to(torch_device)  \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(torch_device), y.to(torch_device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    model.to(torch_device)  \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(torch_device), y.to(torch_device)\n",
    "            pred = model(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training sampled data\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "loss: 2.322161 [   32/  500\n",
      "Accuracy: 40.075626373291016\n",
      "Avg loss: 2.075148 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "loss: 2.119331 [   32/  500\n",
      "Accuracy: 54.48772430419922\n",
      "Avg loss: 1.271622 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "loss: 1.209976 [   32/  500\n",
      "Accuracy: 77.36872100830078\n",
      "Avg loss: 0.777884 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "loss: 0.329156 [   32/  500\n",
      "Accuracy: 81.67094421386719\n",
      "Avg loss: 0.664084 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "loss: 0.185020 [   32/  500\n",
      "Accuracy: 83.6900634765625\n",
      "Avg loss: 0.606769 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "loss: 0.108664 [   32/  500\n",
      "Accuracy: 83.6258544921875\n",
      "Avg loss: 0.614081 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "loss: 0.060232 [   32/  500\n",
      "Accuracy: 84.71746826171875\n",
      "Avg loss: 0.594197 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "loss: 0.033301 [   32/  500\n",
      "Accuracy: 84.28937530517578\n",
      "Avg loss: 0.628388 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "loss: 0.039392 [   32/  500\n",
      "Accuracy: 84.2037582397461\n",
      "Avg loss: 0.658771 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "loss: 0.011788 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.667010 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "loss: 0.012979 [   32/  500\n",
      "Accuracy: 84.50341796875\n",
      "Avg loss: 0.683379 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "loss: 0.004805 [   32/  500\n",
      "Accuracy: 84.52482604980469\n",
      "Avg loss: 0.701546 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "loss: 0.007645 [   32/  500\n",
      "Accuracy: 84.51055145263672\n",
      "Avg loss: 0.713303 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "loss: 0.004168 [   32/  500\n",
      "Accuracy: 84.52482604980469\n",
      "Avg loss: 0.713575 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "loss: 0.005578 [   32/  500\n",
      "Accuracy: 84.56050109863281\n",
      "Avg loss: 0.714408 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "loss: 0.002860 [   32/  500\n",
      "Accuracy: 84.57476806640625\n",
      "Avg loss: 0.714978 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "loss: 0.003210 [   32/  500\n",
      "Accuracy: 84.5819091796875\n",
      "Avg loss: 0.715653 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "loss: 0.002985 [   32/  500\n",
      "Accuracy: 84.56050109863281\n",
      "Avg loss: 0.716740 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "loss: 0.005297 [   32/  500\n",
      "Accuracy: 84.53910064697266\n",
      "Avg loss: 0.717978 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "loss: 0.004008 [   32/  500\n",
      "Accuracy: 84.53910064697266\n",
      "Avg loss: 0.718074 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "loss: 0.004452 [   32/  500\n",
      "Accuracy: 84.54623413085938\n",
      "Avg loss: 0.718168 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "loss: 0.004935 [   32/  500\n",
      "Accuracy: 84.54623413085938\n",
      "Avg loss: 0.718266 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "loss: 0.003494 [   32/  500\n",
      "Accuracy: 84.54623413085938\n",
      "Avg loss: 0.718372 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "loss: 0.003811 [   32/  500\n",
      "Accuracy: 84.5533676147461\n",
      "Avg loss: 0.718460 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "loss: 0.002231 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718579 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "loss: 0.004489 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718590 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "loss: 0.003099 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718600 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "loss: 0.003740 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718616 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "loss: 0.003697 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718626 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "loss: 0.004192 [   32/  500\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718636 \n",
      "\n",
      "Training took 0.08 minutes.\n",
      "Accuracy: 84.56763458251953\n",
      "Avg loss: 0.718636 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training sampled data\\n--------------------------\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_sampled_loader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_loader, model, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "--------------------------\n",
      "Epoch 1\n",
      "--------------------------\n",
      "loss: 0.971602 [   32/56000\n",
      "loss: 0.083260 [ 3232/56000\n",
      "loss: 0.226649 [ 6432/56000\n",
      "loss: 0.138727 [ 9632/56000\n",
      "loss: 0.213784 [12832/56000\n",
      "loss: 0.102214 [16032/56000\n",
      "loss: 0.083280 [19232/56000\n",
      "loss: 0.145916 [22432/56000\n",
      "loss: 0.256179 [25632/56000\n",
      "loss: 0.090833 [28832/56000\n",
      "loss: 0.172571 [32032/56000\n",
      "loss: 0.105045 [35232/56000\n",
      "loss: 0.658285 [38432/56000\n",
      "loss: 0.290153 [41632/56000\n",
      "loss: 0.268519 [44832/56000\n",
      "loss: 0.174157 [48032/56000\n",
      "loss: 0.447456 [51232/56000\n",
      "loss: 0.214540 [54432/56000\n",
      "Accuracy: 95.54793548583984\n",
      "Avg loss: 0.154940 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "loss: 0.142550 [   32/56000\n",
      "loss: 0.469733 [ 3232/56000\n",
      "loss: 0.022658 [ 6432/56000\n",
      "loss: 0.445050 [ 9632/56000\n",
      "loss: 0.244947 [12832/56000\n",
      "loss: 0.058959 [16032/56000\n",
      "loss: 0.160407 [19232/56000\n",
      "loss: 0.153737 [22432/56000\n",
      "loss: 0.147507 [25632/56000\n",
      "loss: 0.147264 [28832/56000\n",
      "loss: 0.330473 [32032/56000\n",
      "loss: 0.013570 [35232/56000\n",
      "loss: 0.069381 [38432/56000\n",
      "loss: 0.018549 [41632/56000\n",
      "loss: 0.057352 [44832/56000\n",
      "loss: 0.082711 [48032/56000\n",
      "loss: 0.028351 [51232/56000\n",
      "loss: 0.067886 [54432/56000\n",
      "Accuracy: 95.62642669677734\n",
      "Avg loss: 0.234431 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "loss: 0.011874 [   32/56000\n",
      "loss: 0.120213 [ 3232/56000\n",
      "loss: 0.063282 [ 6432/56000\n",
      "loss: 0.099435 [ 9632/56000\n",
      "loss: 0.077138 [12832/56000\n",
      "loss: 0.081465 [16032/56000\n",
      "loss: 0.063284 [19232/56000\n",
      "loss: 0.003639 [22432/56000\n",
      "loss: 0.081930 [25632/56000\n",
      "loss: 0.195963 [28832/56000\n",
      "loss: 0.026335 [32032/56000\n",
      "loss: 0.016251 [35232/56000\n",
      "loss: 0.048684 [38432/56000\n",
      "loss: 0.055128 [41632/56000\n",
      "loss: 0.003894 [44832/56000\n",
      "loss: 0.012437 [48032/56000\n",
      "loss: 0.057907 [51232/56000\n",
      "loss: 0.008086 [54432/56000\n",
      "Accuracy: 96.8464584350586\n",
      "Avg loss: 0.226558 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "loss: 0.022008 [   32/56000\n",
      "loss: 0.013756 [ 3232/56000\n",
      "loss: 0.013349 [ 6432/56000\n",
      "loss: 0.015378 [ 9632/56000\n",
      "loss: 0.021546 [12832/56000\n",
      "loss: 0.026822 [16032/56000\n",
      "loss: 0.001858 [19232/56000\n",
      "loss: 0.185994 [22432/56000\n",
      "loss: 0.007678 [25632/56000\n",
      "loss: 0.057155 [28832/56000\n",
      "loss: 0.045529 [32032/56000\n",
      "loss: 0.015878 [35232/56000\n",
      "loss: 0.079751 [38432/56000\n",
      "loss: 0.016564 [41632/56000\n",
      "loss: 0.334368 [44832/56000\n",
      "loss: 0.038966 [48032/56000\n",
      "loss: 0.005167 [51232/56000\n",
      "loss: 0.077379 [54432/56000\n",
      "Accuracy: 96.58247375488281\n",
      "Avg loss: 0.236889 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "loss: 0.117150 [   32/56000\n",
      "loss: 0.003422 [ 3232/56000\n",
      "loss: 0.001749 [ 6432/56000\n",
      "loss: 0.016587 [ 9632/56000\n",
      "loss: 0.018624 [12832/56000\n",
      "loss: 0.000524 [16032/56000\n",
      "loss: 0.043046 [19232/56000\n",
      "loss: 0.016968 [22432/56000\n",
      "loss: 0.091064 [25632/56000\n",
      "loss: 0.016511 [28832/56000\n",
      "loss: 0.029488 [32032/56000\n",
      "loss: 0.057813 [35232/56000\n",
      "loss: 0.019516 [38432/56000\n",
      "loss: 0.022479 [41632/56000\n",
      "loss: 0.021993 [44832/56000\n",
      "loss: 0.005514 [48032/56000\n",
      "loss: 0.002825 [51232/56000\n",
      "loss: 0.193947 [54432/56000\n",
      "Accuracy: 96.75370788574219\n",
      "Avg loss: 0.245543 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------\n",
      "loss: 0.074947 [   32/56000\n",
      "loss: 0.005658 [ 3232/56000\n",
      "loss: 0.067582 [ 6432/56000\n",
      "loss: 0.039214 [ 9632/56000\n",
      "loss: 0.038633 [12832/56000\n",
      "loss: 0.009819 [16032/56000\n",
      "loss: 0.025640 [19232/56000\n",
      "loss: 0.009267 [22432/56000\n",
      "loss: 0.042713 [25632/56000\n",
      "loss: 0.002265 [28832/56000\n",
      "loss: 0.097123 [32032/56000\n",
      "loss: 0.007249 [35232/56000\n",
      "loss: 0.008024 [38432/56000\n",
      "loss: 0.004142 [41632/56000\n",
      "loss: 0.063732 [44832/56000\n",
      "loss: 0.009789 [48032/56000\n",
      "loss: 0.271210 [51232/56000\n",
      "loss: 0.071392 [54432/56000\n",
      "Accuracy: 97.34588623046875\n",
      "Avg loss: 0.284533 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------\n",
      "loss: 0.003999 [   32/56000\n",
      "loss: 0.000762 [ 3232/56000\n",
      "loss: 0.010978 [ 6432/56000\n",
      "loss: 0.013780 [ 9632/56000\n",
      "loss: 0.086653 [12832/56000\n",
      "loss: 0.042584 [16032/56000\n",
      "loss: 0.000100 [19232/56000\n",
      "loss: 0.114952 [22432/56000\n",
      "loss: 0.001124 [25632/56000\n",
      "loss: 0.000729 [28832/56000\n",
      "loss: 0.006032 [32032/56000\n",
      "loss: 0.018777 [35232/56000\n",
      "loss: 0.000106 [38432/56000\n",
      "loss: 0.014345 [41632/56000\n",
      "loss: 0.000726 [44832/56000\n",
      "loss: 0.002732 [48032/56000\n",
      "loss: 0.002724 [51232/56000\n",
      "loss: 0.030411 [54432/56000\n",
      "Accuracy: 97.36729431152344\n",
      "Avg loss: 0.292985 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------\n",
      "loss: 0.002631 [   32/56000\n",
      "loss: 0.033699 [ 3232/56000\n",
      "loss: 0.060568 [ 6432/56000\n",
      "loss: 0.014790 [ 9632/56000\n",
      "loss: 0.001996 [12832/56000\n",
      "loss: 0.000335 [16032/56000\n",
      "loss: 0.002717 [19232/56000\n",
      "loss: 0.156528 [22432/56000\n",
      "loss: 0.001479 [25632/56000\n",
      "loss: 0.017984 [28832/56000\n",
      "loss: 0.002884 [32032/56000\n",
      "loss: 0.026889 [35232/56000\n",
      "loss: 0.008799 [38432/56000\n",
      "loss: 0.005873 [41632/56000\n",
      "loss: 0.000187 [44832/56000\n",
      "loss: 0.000653 [48032/56000\n",
      "loss: 0.002524 [51232/56000\n",
      "loss: 0.261146 [54432/56000\n",
      "Accuracy: 97.75969696044922\n",
      "Avg loss: 0.307366 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------\n",
      "loss: 0.000226 [   32/56000\n",
      "loss: 0.002812 [ 3232/56000\n",
      "loss: 0.000726 [ 6432/56000\n",
      "loss: 0.000726 [ 9632/56000\n",
      "loss: 0.000381 [12832/56000\n",
      "loss: 0.000727 [16032/56000\n",
      "loss: 0.000921 [19232/56000\n",
      "loss: 0.000240 [22432/56000\n",
      "loss: 0.002488 [25632/56000\n",
      "loss: 0.027412 [28832/56000\n",
      "loss: 0.000054 [32032/56000\n",
      "loss: 0.000031 [35232/56000\n",
      "loss: 0.001394 [38432/56000\n",
      "loss: 0.000011 [41632/56000\n",
      "loss: 0.000038 [44832/56000\n",
      "loss: 0.000204 [48032/56000\n",
      "loss: 0.001352 [51232/56000\n",
      "loss: 0.008812 [54432/56000\n",
      "Accuracy: 97.8453140258789\n",
      "Avg loss: 0.339166 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------\n",
      "loss: 0.000217 [   32/56000\n",
      "loss: 0.002365 [ 3232/56000\n",
      "loss: 0.000125 [ 6432/56000\n",
      "loss: 0.000611 [ 9632/56000\n",
      "loss: 0.000449 [12832/56000\n",
      "loss: 0.001534 [16032/56000\n",
      "loss: 0.001259 [19232/56000\n",
      "loss: 0.004932 [22432/56000\n",
      "loss: 0.001768 [25632/56000\n",
      "loss: 0.001216 [28832/56000\n",
      "loss: 0.000753 [32032/56000\n",
      "loss: 0.001370 [35232/56000\n",
      "loss: 0.001034 [38432/56000\n",
      "loss: 0.000621 [41632/56000\n",
      "loss: 0.001732 [44832/56000\n",
      "loss: 0.000235 [48032/56000\n",
      "loss: 0.000008 [51232/56000\n",
      "loss: 0.000476 [54432/56000\n",
      "Accuracy: 97.85958862304688\n",
      "Avg loss: 0.380114 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------\n",
      "loss: 0.001398 [   32/56000\n",
      "loss: 0.000215 [ 3232/56000\n",
      "loss: 0.000383 [ 6432/56000\n",
      "loss: 0.056712 [ 9632/56000\n",
      "loss: 0.000417 [12832/56000\n",
      "loss: 0.000071 [16032/56000\n",
      "loss: 0.003620 [19232/56000\n",
      "loss: 0.026388 [22432/56000\n",
      "loss: 0.000043 [25632/56000\n",
      "loss: 0.001562 [28832/56000\n",
      "loss: 0.000059 [32032/56000\n",
      "loss: 0.000200 [35232/56000\n",
      "loss: 0.000040 [38432/56000\n",
      "loss: 0.000192 [41632/56000\n",
      "loss: 0.000095 [44832/56000\n",
      "loss: 0.000993 [48032/56000\n",
      "loss: 0.000037 [51232/56000\n",
      "loss: 0.001580 [54432/56000\n",
      "Accuracy: 97.90238952636719\n",
      "Avg loss: 0.420552 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------\n",
      "loss: 0.000470 [   32/56000\n",
      "loss: 0.000113 [ 3232/56000\n",
      "loss: 0.000004 [ 6432/56000\n",
      "loss: 0.000552 [ 9632/56000\n",
      "loss: 0.000089 [12832/56000\n",
      "loss: 0.000193 [16032/56000\n",
      "loss: 0.001154 [19232/56000\n",
      "loss: 0.001136 [22432/56000\n",
      "loss: 0.000370 [25632/56000\n",
      "loss: 0.002067 [28832/56000\n",
      "loss: 0.000084 [32032/56000\n",
      "loss: 0.000031 [35232/56000\n",
      "loss: 0.000102 [38432/56000\n",
      "loss: 0.000116 [41632/56000\n",
      "loss: 0.000004 [44832/56000\n",
      "loss: 0.000334 [48032/56000\n",
      "loss: 0.000720 [51232/56000\n",
      "loss: 0.000074 [54432/56000\n",
      "Accuracy: 97.83818054199219\n",
      "Avg loss: 0.464079 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------\n",
      "loss: 0.000011 [   32/56000\n",
      "loss: 0.002414 [ 3232/56000\n",
      "loss: 0.000053 [ 6432/56000\n",
      "loss: 0.000005 [ 9632/56000\n",
      "loss: 0.000031 [12832/56000\n",
      "loss: 0.000066 [16032/56000\n",
      "loss: 0.000153 [19232/56000\n",
      "loss: 0.005459 [22432/56000\n",
      "loss: 0.000010 [25632/56000\n",
      "loss: 0.000028 [28832/56000\n",
      "loss: 0.000169 [32032/56000\n",
      "loss: 0.000602 [35232/56000\n",
      "loss: 0.000061 [38432/56000\n",
      "loss: 0.000003 [41632/56000\n",
      "loss: 0.000052 [44832/56000\n",
      "loss: 0.000008 [48032/56000\n",
      "loss: 0.000251 [51232/56000\n",
      "loss: 0.000016 [54432/56000\n",
      "Accuracy: 97.91666412353516\n",
      "Avg loss: 0.504282 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------\n",
      "loss: 0.000063 [   32/56000\n",
      "loss: 0.000027 [ 3232/56000\n",
      "loss: 0.000724 [ 6432/56000\n",
      "loss: 0.000141 [ 9632/56000\n",
      "loss: 0.000591 [12832/56000\n",
      "loss: 0.000262 [16032/56000\n",
      "loss: 0.000027 [19232/56000\n",
      "loss: 0.000829 [22432/56000\n",
      "loss: 0.000634 [25632/56000\n",
      "loss: 0.000879 [28832/56000\n",
      "loss: 0.000038 [32032/56000\n",
      "loss: 0.000015 [35232/56000\n",
      "loss: 0.001934 [38432/56000\n",
      "loss: 0.000182 [41632/56000\n",
      "loss: 0.000006 [44832/56000\n",
      "loss: 0.000086 [48032/56000\n",
      "loss: 0.000509 [51232/56000\n",
      "loss: 0.000244 [54432/56000\n",
      "Accuracy: 97.90238952636719\n",
      "Avg loss: 0.508591 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------\n",
      "loss: 0.000000 [   32/56000\n",
      "loss: 0.000023 [ 3232/56000\n",
      "loss: 0.000016 [ 6432/56000\n",
      "loss: 0.000163 [ 9632/56000\n",
      "loss: 0.000019 [12832/56000\n",
      "loss: 0.000004 [16032/56000\n",
      "loss: 0.003948 [19232/56000\n",
      "loss: 0.000410 [22432/56000\n",
      "loss: 0.000087 [25632/56000\n",
      "loss: 0.000182 [28832/56000\n",
      "loss: 0.001084 [32032/56000\n",
      "loss: 0.000018 [35232/56000\n",
      "loss: 0.000010 [38432/56000\n",
      "loss: 0.000062 [41632/56000\n",
      "loss: 0.000031 [44832/56000\n",
      "loss: 0.000001 [48032/56000\n",
      "loss: 0.000082 [51232/56000\n",
      "loss: 0.000122 [54432/56000\n",
      "Accuracy: 97.91666412353516\n",
      "Avg loss: 0.517703 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------\n",
      "loss: 0.000221 [   32/56000\n",
      "loss: 0.001602 [ 3232/56000\n",
      "loss: 0.000004 [ 6432/56000\n",
      "loss: 0.000011 [ 9632/56000\n",
      "loss: 0.000115 [12832/56000\n",
      "loss: 0.000014 [16032/56000\n",
      "loss: 0.000178 [19232/56000\n",
      "loss: 0.000112 [22432/56000\n",
      "loss: 0.000268 [25632/56000\n",
      "loss: 0.000006 [28832/56000\n",
      "loss: 0.000000 [32032/56000\n",
      "loss: 0.000185 [35232/56000\n",
      "loss: 0.000208 [38432/56000\n",
      "loss: 0.000684 [41632/56000\n",
      "loss: 0.000004 [44832/56000\n",
      "loss: 0.000131 [48032/56000\n",
      "loss: 0.000004 [51232/56000\n",
      "loss: 0.000024 [54432/56000\n",
      "Accuracy: 97.92379760742188\n",
      "Avg loss: 0.530170 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------\n",
      "loss: 0.000108 [   32/56000\n",
      "loss: 0.000009 [ 3232/56000\n",
      "loss: 0.001209 [ 6432/56000\n",
      "loss: 0.000001 [ 9632/56000\n",
      "loss: 0.000014 [12832/56000\n",
      "loss: 0.000002 [16032/56000\n",
      "loss: 0.000138 [19232/56000\n",
      "loss: 0.000141 [22432/56000\n",
      "loss: 0.000001 [25632/56000\n",
      "loss: 0.000001 [28832/56000\n",
      "loss: 0.000097 [32032/56000\n",
      "loss: 0.000072 [35232/56000\n",
      "loss: 0.000013 [38432/56000\n",
      "loss: 0.000018 [41632/56000\n",
      "loss: 0.000396 [44832/56000\n",
      "loss: 0.000021 [48032/56000\n",
      "loss: 0.000005 [51232/56000\n",
      "loss: 0.000012 [54432/56000\n",
      "Accuracy: 97.90238952636719\n",
      "Avg loss: 0.541892 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------\n",
      "loss: 0.000031 [   32/56000\n",
      "loss: 0.000389 [ 3232/56000\n",
      "loss: 0.000032 [ 6432/56000\n",
      "loss: 0.000227 [ 9632/56000\n",
      "loss: 0.000006 [12832/56000\n",
      "loss: 0.000003 [16032/56000\n",
      "loss: 0.000009 [19232/56000\n",
      "loss: 0.000109 [22432/56000\n",
      "loss: 0.000104 [25632/56000\n",
      "loss: 0.003026 [28832/56000\n",
      "loss: 0.000561 [32032/56000\n",
      "loss: 0.000239 [35232/56000\n",
      "loss: 0.000188 [38432/56000\n",
      "loss: 0.000133 [41632/56000\n",
      "loss: 0.000003 [44832/56000\n",
      "loss: 0.000214 [48032/56000\n",
      "loss: 0.000001 [51232/56000\n",
      "loss: 0.000003 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.554260 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------\n",
      "loss: 0.000054 [   32/56000\n",
      "loss: 0.000078 [ 3232/56000\n",
      "loss: 0.000000 [ 6432/56000\n",
      "loss: 0.000096 [ 9632/56000\n",
      "loss: 0.000051 [12832/56000\n",
      "loss: 0.000148 [16032/56000\n",
      "loss: 0.000031 [19232/56000\n",
      "loss: 0.000040 [22432/56000\n",
      "loss: 0.000000 [25632/56000\n",
      "loss: 0.000001 [28832/56000\n",
      "loss: 0.000084 [32032/56000\n",
      "loss: 0.000003 [35232/56000\n",
      "loss: 0.000077 [38432/56000\n",
      "loss: 0.000001 [41632/56000\n",
      "loss: 0.000169 [44832/56000\n",
      "loss: 0.000001 [48032/56000\n",
      "loss: 0.000088 [51232/56000\n",
      "loss: 0.000265 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.565633 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------\n",
      "loss: 0.000000 [   32/56000\n",
      "loss: 0.000005 [ 3232/56000\n",
      "loss: 0.000021 [ 6432/56000\n",
      "loss: 0.000035 [ 9632/56000\n",
      "loss: 0.000001 [12832/56000\n",
      "loss: 0.000083 [16032/56000\n",
      "loss: 0.010983 [19232/56000\n",
      "loss: 0.001847 [22432/56000\n",
      "loss: 0.000002 [25632/56000\n",
      "loss: 0.000066 [28832/56000\n",
      "loss: 0.000098 [32032/56000\n",
      "loss: 0.000058 [35232/56000\n",
      "loss: 0.000127 [38432/56000\n",
      "loss: 0.000561 [41632/56000\n",
      "loss: 0.000002 [44832/56000\n",
      "loss: 0.000025 [48032/56000\n",
      "loss: 0.000001 [51232/56000\n",
      "loss: 0.000001 [54432/56000\n",
      "Accuracy: 97.92379760742188\n",
      "Avg loss: 0.566979 \n",
      "\n",
      "Epoch 21\n",
      "--------------------------\n",
      "loss: 0.000020 [   32/56000\n",
      "loss: 0.000235 [ 3232/56000\n",
      "loss: 0.000053 [ 6432/56000\n",
      "loss: 0.000000 [ 9632/56000\n",
      "loss: 0.000017 [12832/56000\n",
      "loss: 0.000066 [16032/56000\n",
      "loss: 0.000004 [19232/56000\n",
      "loss: 0.000050 [22432/56000\n",
      "loss: 0.000017 [25632/56000\n",
      "loss: 0.000064 [28832/56000\n",
      "loss: 0.000054 [32032/56000\n",
      "loss: 0.000001 [35232/56000\n",
      "loss: 0.000330 [38432/56000\n",
      "loss: 0.000005 [41632/56000\n",
      "loss: 0.000017 [44832/56000\n",
      "loss: 0.000072 [48032/56000\n",
      "loss: 0.000032 [51232/56000\n",
      "loss: 0.000020 [54432/56000\n",
      "Accuracy: 97.92379760742188\n",
      "Avg loss: 0.568427 \n",
      "\n",
      "Epoch 22\n",
      "--------------------------\n",
      "loss: 0.000104 [   32/56000\n",
      "loss: 0.000023 [ 3232/56000\n",
      "loss: 0.062628 [ 6432/56000\n",
      "loss: 0.000002 [ 9632/56000\n",
      "loss: 0.000146 [12832/56000\n",
      "loss: 0.000009 [16032/56000\n",
      "loss: 0.000005 [19232/56000\n",
      "loss: 0.000002 [22432/56000\n",
      "loss: 0.000108 [25632/56000\n",
      "loss: 0.000001 [28832/56000\n",
      "loss: 0.000055 [32032/56000\n",
      "loss: 0.000224 [35232/56000\n",
      "loss: 0.000755 [38432/56000\n",
      "loss: 0.000560 [41632/56000\n",
      "loss: 0.000135 [44832/56000\n",
      "loss: 0.000000 [48032/56000\n",
      "loss: 0.000015 [51232/56000\n",
      "loss: 0.000003 [54432/56000\n",
      "Accuracy: 97.91666412353516\n",
      "Avg loss: 0.569873 \n",
      "\n",
      "Epoch 23\n",
      "--------------------------\n",
      "loss: 0.000021 [   32/56000\n",
      "loss: 0.000001 [ 3232/56000\n",
      "loss: 0.000017 [ 6432/56000\n",
      "loss: 0.002957 [ 9632/56000\n",
      "loss: 0.000059 [12832/56000\n",
      "loss: 0.000432 [16032/56000\n",
      "loss: 0.000005 [19232/56000\n",
      "loss: 0.000011 [22432/56000\n",
      "loss: 0.000035 [25632/56000\n",
      "loss: 0.000000 [28832/56000\n",
      "loss: 0.000457 [32032/56000\n",
      "loss: 0.000175 [35232/56000\n",
      "loss: 0.000002 [38432/56000\n",
      "loss: 0.000000 [41632/56000\n",
      "loss: 0.000000 [44832/56000\n",
      "loss: 0.000022 [48032/56000\n",
      "loss: 0.000015 [51232/56000\n",
      "loss: 0.000010 [54432/56000\n",
      "Accuracy: 97.92379760742188\n",
      "Avg loss: 0.571434 \n",
      "\n",
      "Epoch 24\n",
      "--------------------------\n",
      "loss: 0.000011 [   32/56000\n",
      "loss: 0.000072 [ 3232/56000\n",
      "loss: 0.000064 [ 6432/56000\n",
      "loss: 0.001667 [ 9632/56000\n",
      "loss: 0.000035 [12832/56000\n",
      "loss: 0.000194 [16032/56000\n",
      "loss: 0.000000 [19232/56000\n",
      "loss: 0.000357 [22432/56000\n",
      "loss: 0.000218 [25632/56000\n",
      "loss: 0.000001 [28832/56000\n",
      "loss: 0.000015 [32032/56000\n",
      "loss: 0.000004 [35232/56000\n",
      "loss: 0.000003 [38432/56000\n",
      "loss: 0.000040 [41632/56000\n",
      "loss: 0.000002 [44832/56000\n",
      "loss: 0.000014 [48032/56000\n",
      "loss: 0.000000 [51232/56000\n",
      "loss: 0.000251 [54432/56000\n",
      "Accuracy: 97.92379760742188\n",
      "Avg loss: 0.572826 \n",
      "\n",
      "Epoch 25\n",
      "--------------------------\n",
      "loss: 0.000362 [   32/56000\n",
      "loss: 0.000001 [ 3232/56000\n",
      "loss: 0.000077 [ 6432/56000\n",
      "loss: 0.000067 [ 9632/56000\n",
      "loss: 0.000240 [12832/56000\n",
      "loss: 0.000016 [16032/56000\n",
      "loss: 0.000012 [19232/56000\n",
      "loss: 0.000028 [22432/56000\n",
      "loss: 0.000015 [25632/56000\n",
      "loss: 0.000050 [28832/56000\n",
      "loss: 0.000027 [32032/56000\n",
      "loss: 0.000034 [35232/56000\n",
      "loss: 0.000009 [38432/56000\n",
      "loss: 0.000137 [41632/56000\n",
      "loss: 0.000102 [44832/56000\n",
      "loss: 0.000000 [48032/56000\n",
      "loss: 0.000002 [51232/56000\n",
      "loss: 0.000066 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574192 \n",
      "\n",
      "Epoch 26\n",
      "--------------------------\n",
      "loss: 0.000101 [   32/56000\n",
      "loss: 0.000021 [ 3232/56000\n",
      "loss: 0.000121 [ 6432/56000\n",
      "loss: 0.007570 [ 9632/56000\n",
      "loss: 0.000048 [12832/56000\n",
      "loss: 0.000437 [16032/56000\n",
      "loss: 0.000054 [19232/56000\n",
      "loss: 0.000021 [22432/56000\n",
      "loss: 0.000000 [25632/56000\n",
      "loss: 0.000021 [28832/56000\n",
      "loss: 0.000008 [32032/56000\n",
      "loss: 0.000023 [35232/56000\n",
      "loss: 0.000064 [38432/56000\n",
      "loss: 0.000220 [41632/56000\n",
      "loss: 0.000005 [44832/56000\n",
      "loss: 0.000054 [48032/56000\n",
      "loss: 0.000043 [51232/56000\n",
      "loss: 0.000028 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574324 \n",
      "\n",
      "Epoch 27\n",
      "--------------------------\n",
      "loss: 0.000169 [   32/56000\n",
      "loss: 0.000016 [ 3232/56000\n",
      "loss: 0.000002 [ 6432/56000\n",
      "loss: 0.000001 [ 9632/56000\n",
      "loss: 0.000001 [12832/56000\n",
      "loss: 0.000009 [16032/56000\n",
      "loss: 0.000034 [19232/56000\n",
      "loss: 0.000001 [22432/56000\n",
      "loss: 0.000001 [25632/56000\n",
      "loss: 0.000013 [28832/56000\n",
      "loss: 0.000010 [32032/56000\n",
      "loss: 0.000475 [35232/56000\n",
      "loss: 0.000053 [38432/56000\n",
      "loss: 0.000005 [41632/56000\n",
      "loss: 0.000031 [44832/56000\n",
      "loss: 0.000002 [48032/56000\n",
      "loss: 0.000000 [51232/56000\n",
      "loss: 0.000033 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574453 \n",
      "\n",
      "Epoch 28\n",
      "--------------------------\n",
      "loss: 0.000008 [   32/56000\n",
      "loss: 0.000066 [ 3232/56000\n",
      "loss: 0.000003 [ 6432/56000\n",
      "loss: 0.000203 [ 9632/56000\n",
      "loss: 0.000017 [12832/56000\n",
      "loss: 0.000002 [16032/56000\n",
      "loss: 0.000002 [19232/56000\n",
      "loss: 0.000003 [22432/56000\n",
      "loss: 0.000004 [25632/56000\n",
      "loss: 0.000003 [28832/56000\n",
      "loss: 0.000189 [32032/56000\n",
      "loss: 0.000075 [35232/56000\n",
      "loss: 0.000019 [38432/56000\n",
      "loss: 0.000001 [41632/56000\n",
      "loss: 0.000009 [44832/56000\n",
      "loss: 0.000019 [48032/56000\n",
      "loss: 0.000069 [51232/56000\n",
      "loss: 0.000042 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574586 \n",
      "\n",
      "Epoch 29\n",
      "--------------------------\n",
      "loss: 0.000032 [   32/56000\n",
      "loss: 0.000016 [ 3232/56000\n",
      "loss: 0.000005 [ 6432/56000\n",
      "loss: 0.000035 [ 9632/56000\n",
      "loss: 0.000018 [12832/56000\n",
      "loss: 0.000066 [16032/56000\n",
      "loss: 0.000002 [19232/56000\n",
      "loss: 0.000047 [22432/56000\n",
      "loss: 0.000479 [25632/56000\n",
      "loss: 0.000044 [28832/56000\n",
      "loss: 0.000144 [32032/56000\n",
      "loss: 0.000076 [35232/56000\n",
      "loss: 0.000008 [38432/56000\n",
      "loss: 0.000004 [41632/56000\n",
      "loss: 0.000015 [44832/56000\n",
      "loss: 0.000100 [48032/56000\n",
      "loss: 0.000000 [51232/56000\n",
      "loss: 0.000158 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574716 \n",
      "\n",
      "Epoch 30\n",
      "--------------------------\n",
      "loss: 0.000003 [   32/56000\n",
      "loss: 0.000007 [ 3232/56000\n",
      "loss: 0.000149 [ 6432/56000\n",
      "loss: 0.000002 [ 9632/56000\n",
      "loss: 0.000136 [12832/56000\n",
      "loss: 0.003172 [16032/56000\n",
      "loss: 0.000000 [19232/56000\n",
      "loss: 0.000032 [22432/56000\n",
      "loss: 0.005352 [25632/56000\n",
      "loss: 0.000042 [28832/56000\n",
      "loss: 0.000006 [32032/56000\n",
      "loss: 0.000001 [35232/56000\n",
      "loss: 0.000028 [38432/56000\n",
      "loss: 0.000032 [41632/56000\n",
      "loss: 0.000044 [44832/56000\n",
      "loss: 0.000185 [48032/56000\n",
      "loss: 0.000187 [51232/56000\n",
      "loss: 0.000006 [54432/56000\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574854 \n",
      "\n",
      "Training took 1.29 minutes.\n",
      "Accuracy: 97.9309310913086\n",
      "Avg loss: 0.574854 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training\\n--------------------------\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_loader, model, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
